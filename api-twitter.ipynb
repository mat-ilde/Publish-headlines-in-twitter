{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that scrapes the \"El país\" page and returns the headlines.\n",
    "def get_headlines():\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    headlines=list()\n",
    "    url=\"https://elpais.com\"\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        headlines.append(text)\n",
    "    return headlines\n",
    "headlines_1=get_headlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way I get all the headlines.\n",
    "# without use a function\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "url=\"https://elpais.com\"\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html, \"html.parser\")\n",
    "s=soup.find_all(class_=\"headline\")\n",
    "for i in s:\n",
    "    i=str(i)\n",
    "    j=i.split(\">\")\n",
    "    m=j[2]\n",
    "    final=m.split(\"</a\")\n",
    "    text=final[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape \"El país\" page get headlines\n",
    "# and post the first headline on twitter.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info('Iniciando el proceso...')\n",
    "def get_headlines(url):\n",
    "    logging.info('Leyendo URL '  +  url)\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        post='\"'+text+'\"'+\" . \"+\"El País\"\n",
    "        return post\n",
    "url=\"https://elpais.es\"\n",
    "titular=get_headlines(url)\n",
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "api = tweepy.API(auth)\n",
    "try:\n",
    "    result=api.update_status(titular)\n",
    "except Exception as e:\n",
    "    logging.error(\"Titular duplicado, no se pudo publicar\", exc_info=False)\n",
    "logging.info('proceso Finalizado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test query to insert in databases.\n",
    "import sqlite3\n",
    "import os.path         # this is the name of databases file.\n",
    "conn = sqlite3.connect('titulares.sqlite3') \n",
    "BASE_DIR = os.path.dirname(os.path.abspath('titulares.sqlite3'))\n",
    "db_path = os.path.join(BASE_DIR,'titulares.sqlite3' )\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    cur = conn.cursor()\n",
    "                # in this step i create the table with the information that i want have.\n",
    "    cur.execute('INSERT INTO titulares (\"titular\",\"fecha\",\"orden\",\"fuente\") VALUES ( ?, ?, ?, ?)', \n",
    "        ( 'crisis coronavirus', '2020-05-05 12:23:34','3','El país' ) )\n",
    "    conn.commit()\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test list before use the headlines list.\n",
    "headlines=list()\n",
    "headlines_0=list()\n",
    "headlines_0.append(\"La vida es solo una\")\n",
    "headlines.append(\"Ama como si no hubiera mañana\")\n",
    "headlines_0.append(\"Todo los días es un día para estar feliz\")\n",
    "headlines.append(\"vive como el último de tu vida\")\n",
    "headlines_0.append(\"Trata como te gustaría que te traten\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get current time and date. \n",
    "def get_current_fecha():\n",
    "    import datetime\n",
    "    x=datetime.datetime.now()\n",
    "    fecha=((x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    return fecha\n",
    "current_fecha=get_current_fecha()\n",
    "print(current_fecha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that scrapes the \"El país\" page and returns the headlines.\n",
    "def get_headlines():\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    headlines=list()\n",
    "    url=\"https://elpais.com\"\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        headlines.append(text)\n",
    "    return headlines\n",
    "headlines_1=get_headlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the algorithm to post to twitter using python.\n",
    "# using this i can write notices messages.\n",
    "# in my case i write messages like \"the process is starting.\"\n",
    "# if i have an error.\n",
    "# and when the process is finished.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "api = tweepy.API(auth)\n",
    "try:                         # here i write what i want to publish.\n",
    "    result=api.update_status(\"aqui lo que quiero publicar\")\n",
    "    logging.info('proceso Finalizado.')\n",
    "except Exception as e:\n",
    "    logging.error(\"Titular duplicado, no se pudo publicar\", exc_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the sameas the previous one.\n",
    "# in this case i use a function.\n",
    "def publish_in_twitter(list_,position):\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "    import tweepy\n",
    "    titular=list_[position]\n",
    "    auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "    auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        result=api.update_status(titular)\n",
    "        logging.info('publicación exitosa.')\n",
    "    except Exception as e:\n",
    "        logging.error(\"Titular duplicado, no se pudo publicar\", exc_info=False)\n",
    "publish=publish_in_twitter(headlines_0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the final function\n",
    "# in this function i scrabble the headlines from the web page.\n",
    "# in the another step i get the current hour\n",
    "# and the next step i intoduce the data inside of databases\n",
    "# and the last step i post in this case the first hadlines.\n",
    "# in this project i use a crontab to publish the automatic form\n",
    "# for this reason i have too a publish.log.\n",
    "# where send me thr logs.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info('Iniciando el proceso...')\n",
    "def get_headlines():\n",
    "    logging.info('Escrapeando página web...')\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    headlines=list()\n",
    "    url=\"https://elpais.com\"\n",
    "    html=urllib.request.urlopen(url).read()\n",
    "    soup=BeautifulSoup(html, \"html.parser\")\n",
    "    s=soup.find_all(class_=\"headline\")\n",
    "    for i in s:\n",
    "        i=str(i)\n",
    "        j=i.split(\">\")\n",
    "        m=j[2]\n",
    "        final=m.split(\"</a\")\n",
    "        text=final[0]\n",
    "        headlines.append(text)\n",
    "    return headlines\n",
    "headlines_1=get_headlines()\n",
    "# función que me da la hora\n",
    "def get_current_fecha():\n",
    "    import datetime\n",
    "    x=datetime.datetime.now()\n",
    "    fecha=((x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    return fecha\n",
    "# función que me mete los datos en la base de datos\n",
    "def insert_headlines_into_db(headlines_1):\n",
    "    logging.info('Obteniendo fecha y hora actual...')\n",
    "    logging.info('Insertando datos en base de datos...')\n",
    "    import sqlite3\n",
    "    import os.path\n",
    "    import datetime\n",
    "    conn = sqlite3.connect('/home/mati/Documentos/api-twitter/titulares.sqlite3')\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath('/home/mati/Documentos/api-twitter/titulares.sqlite3'))\n",
    "    db_path = os.path.join(BASE_DIR,'/home/mati/Documentos/api-twitter/titulares.sqlite3' )\n",
    "    current_fecha=get_current_fecha()\n",
    "    for value,titular in enumerate(headlines_1):\n",
    "        value=value+1\n",
    "        with sqlite3.connect(db_path) as db:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute('INSERT INTO titulares (\"titular\",\"fecha\",\"orden\",\"fuente\") VALUES ( ?, ?, ?, ?)', \n",
    "            ( titular, current_fecha,value,'El país' ) )\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "    logging.info('Proceso de base de datos finalizado.')\n",
    "    conn.close()\n",
    "insert_headlines_into_db(headlines_1)\n",
    "def publish_in_twitter(list_,position=0):\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "    import tweepy\n",
    "    titular=list_[position]\n",
    "    auth = tweepy.OAuthHandler(\"BtHNJMURBTjLgq5WTBJqKRZ92\",\"MZfyrMJKeriYZu7vufZOZrwklmNYfZ2gaGuqUTGzBR6xcXt7vD\")\n",
    "    auth.set_access_token(\"2766462103-gSIyudYqMhYNIfwFZS82Dm5tXqtCrf2opkj0kmP\",\"UkEr4QKUjVIOTEBmF7iBoNNjFnTk4e3eHR4M6bTruOdVK\")\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        result=api.update_status(titular)\n",
    "        logging.info('Publicación exitosa.')\n",
    "    except Exception as e:\n",
    "        logging.error(\"Titular duplicado no se pudo publicar.\", exc_info=False)\n",
    "publish=publish_in_twitter(headlines_1,6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
